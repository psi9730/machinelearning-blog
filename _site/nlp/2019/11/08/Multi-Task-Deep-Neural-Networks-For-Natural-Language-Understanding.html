<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- Favicon Icon -->
    <link rel="shortcut icon" type="image/x-icon" href="/machinelearning-blog/assets/images/favicon.png">

    <title>Multi Task Deep Neural Networks for Natural Language Understanding</title>
    <meta name="description"
          content="Paper review on “Multi Task Deep Neural Networks for Natural Language Understanding(2019-1-31)”">

    <link rel="canonical" href="http://psi9730.github.io/machinelearning-blog/nlp/2019/11/08/Multi-Task-Deep-Neural-Networks-For-Natural-Language-Understanding.html">
    <link rel="alternate" type="application/rss+xml" title="Seungil's development Blog" href="http://psi9730.github.io/machinelearning-blog/feed.xml">

    <script type="text/javascript" src="/machinelearning-blog/bower_components/jquery/jquery.min.js"></script>

    <!-- Third-Party CSS -->
    <link rel="stylesheet" href="/machinelearning-blog/bower_components/bootstrap/dist/css/bootstrap.min.css">
    <link rel="stylesheet" href="/machinelearning-blog/bower_components/octicons/octicons/octicons.css">
    <link rel="stylesheet" href="/machinelearning-blog/bower_components/hover/css/hover-min.css">
    <link rel="stylesheet" href="/machinelearning-blog/bower_components/primer-markdown/dist/user-content.min.css">
    <link rel="stylesheet" href="/machinelearning-blog/assets/css/syntax.css">

    <!-- My CSS -->
    <link rel="stylesheet" href="/machinelearning-blog/assets/css/common.css">

    <!-- CSS set in page -->
    

    <!-- CSS set in layout -->
    
    <link rel="stylesheet" href="/machinelearning-blog/assets/css/sidebar-post-nav.css">
    

    <script type="text/javascript" src="/machinelearning-blog/bower_components/bootstrap/dist/js/bootstrap.min.js"></script>

</head>


    <body>

    <header class="site-header">
    <div class="container">
        <a id="site-header-brand" href="/machinelearning-blog/" title="Park Seungil">
            <span class="octicon octicon-mark-github"></span> Park Seungil
        </a>
        <nav class="site-header-nav" role="navigation">
            
            <a href="/machinelearning-blog/"
               class=" site-header-nav-item hvr-underline-from-center"
               target=""
               title="Home">
                Home
            </a>
            
            <a href="/machinelearning-blog/open-source"
               class=" site-header-nav-item hvr-underline-from-center"
               target=""
               title="Open-Source">
                Open-Source
            </a>
            
            <a href="/machinelearning-blog/blog"
               class=" site-header-nav-item hvr-underline-from-center"
               target=""
               title="Blog">
                Blog
            </a>
            
            <a href="/machinelearning-blog/bookmark"
               class=" site-header-nav-item hvr-underline-from-center"
               target=""
               title="Bookmark">
                Bookmark
            </a>
            
            <a href="/machinelearning-blog/about"
               class=" site-header-nav-item hvr-underline-from-center"
               target=""
               title="About">
                About
            </a>
            
        </nav>
    </div>
</header>


        <div class="content">
            <section class="jumbotron geopattern" data-pattern-id="Multi Task Deep Neural Networks for Natural Language Understanding">
    <div class="container">
        <div id="jumbotron-meta-info">
            <h1>Multi Task Deep Neural Networks for Natural Language Understanding</h1>
            <span class="meta-info">
                
                
                <span class="octicon octicon-calendar"></span> 2019/11/08
                
            </span>
        </div>
    </div>
</section>
<script>
    $(document).ready(function(){

        $('.geopattern').each(function(){
            $(this).geopattern($(this).data('pattern-id'));
        });

    });
</script>
<article class="post container" itemscope itemtype="http://schema.org/BlogPosting">

    <div class="row">

        
        <div class="col-md-12 markdown-body">

            <p>Paper review on “Multi Task Deep Neural Networks for Natural Language Understanding(2019-1-31)”</p>

<h2 id="abstract">Abstract</h2>
<ul>
  <li>Multi-task Deep Neural Network for learning representations accross multiple NLU tasks</li>
  <li>BERT에 multi task learning을 적용하여 regularization effect + 특정 task에 대한 fine-tuning에 필요한 데이터양을 줄여주며 + 성능향상까지</li>
  <li>representaion을 Language model by unsupervised data + multi-task learning by supervised data 으로한다는데 가장 큰 의의가 있다</li>
</ul>

<h2 id="introduction">Introduction</h2>
<ul>
  <li>MTL을 이용하면 사람이 스키를 탈줄알면 스케이팅을 배우기 쉽듯이 다른 task에 대해 긍정적 영향을 준다.</li>
  <li>특정 task에 대해 overfitting 되지 않아 regularization효과를 가진다.</li>
  <li>MT-DNN 이후에 task-specific labeled data가 적어도 fine-tuning이 잘된다.</li>
  <li>두번의 fine-tuning과정 1. MTL 2. task specific fine tuning 하지만 논문에서는 MTL은 fine tuning이라 칭하지는 않는것같다??</li>
</ul>

<h2 id="tasks">Tasks</h2>
<p>MT-DNN 다음과 같은 4가지 objectives를 가진다.</p>

<ul>
  <li>single-sentence classification
    <ul>
      <li>COLA: 문법적으로 맞는지 Binary classification</li>
      <li>SST-2: 긍정 부정 Binary Classification</li>
    </ul>
  </li>
  <li>pairwise text classification
    <ul>
      <li>RTE, MNLI: entailment, contradiction or neutral 예측하기</li>
      <li>QQP, MRPC: 두개의 문장이 의미적으로 같은지</li>
    </ul>
  </li>
  <li>text similarity scoring
    <ul>
      <li>STS-B: 두 문장 사이 유사도(0~1)</li>
    </ul>
  </li>
  <li>relevance ranking
    <ul>
      <li>QNLI: questions에 대한 정답이 paragraph에 있는가, 해당 논문에서는 pairwise-ranking task로 풀어 성능을 올렸다고 한다.</li>
    </ul>
  </li>
</ul>

<h2 id="the-proposed-mt-dnn-model">The proposed MT-DNN Model</h2>
<ul>
  <li>
    <p>Lexical Encoder <br />
BERT와 같은 구조를 가진다. word-piece embedding, segment embedding, position embedding 3가지 embedding을 이용하며 문장 맨앞엔 [CLS], 문장쌍을 이용할 땐 [SEP]토큰을 이용한다.</p>
  </li>
  <li>Transformer Encoder<br />
pre training을 하는 부분인데 모든 task에서 공유하는 부분이다. BERT모델은 이부분을 오로지 unsupervised pre-training하는 반면에, MT-DNN은 BERT의 pre-training에 이어서 multi-task objectives을 이용하여 학습시킨다. 이를 통해 보다 #enerative한 representation을 만들 수 있다.</li>
  <li>Single-Sentence Classification Output<br />
classification by softmax on [CLS] representation<br />
<img src="/machinelearning-blog/assets/images/2019-11-08-Multi-Task-Deep-Neural-Networks-For-Natural-Language-Understanding-6.png" alt="image-6" /></li>
  <li>Text Similarity Output<br />
get regression data by sigmoid on [CLS] representation (0~1)<br />
<img src="/machinelearning-blog/assets/images/2019-11-08-Multi-Task-Deep-Neural-Networks-For-Natural-Language-Understanding-7.png" alt="image-7" /></li>
  <li>Pairwise Text Classification Output<br />
문장 쌍에서의 관계를 살펴볼때 한번 씩 문장을 봐서는 전체의 맥락을 파악하지 못할 수 있다에서 근거하여, GRU의 input에는 premise, hidden state에는 hypothesis 문장을 넣는다.<br />
<img src="/machinelearning-blog/assets/images/2019-11-08-Multi-Task-Deep-Neural-Networks-For-Natural-Language-Understanding-8.png" alt="image-8" /></li>
  <li>Relevance Ranking Output<br />
Question에 대한 답이 paragraph에 있는지 여부를 binary로 찾는 문제를, paragraph안에 모든 예상 answer에 대한 relevance score를 구해 문제를 해결한다.<br />
<img src="/machinelearning-blog/assets/images/2019-11-08-Multi-Task-Deep-Neural-Networks-For-Natural-Language-Understanding-9.png" alt="image-9" /></li>
</ul>

<h2 id="the-training-procedure">The Training Procedure</h2>
<p><img src="/machinelearning-blog/assets/images/2019-11-08-Multi-Task-Deep-Neural-Networks-For-Natural-Language-Understanding-1.png" alt="image-1" /><br />
Pretrained 된 BERT model의 shared layers에다가 MTL을 SGD로 학습한다. <br />
shared layers, task specific layers를 동시에 학습한다.<br />
데이터는 모두 합친 뒤 shuffle한 다음 batch만큼 가져오는 식으로.<br />
3가지 objectives가 있다. <br />
- (6): cross entroy loss for single-sentence, pairwise text classification<br />
- (7): mean squared error for text similarity tasks<br />
- (8),(9): minimize negative log likelihood of positive example for relevance ranking tasks</p>

<p><img src="/machinelearning-blog/assets/images/2019-11-08-Multi-Task-Deep-Neural-Networks-For-Natural-Language-Understanding-5.png" alt="image-2" /><br />
<img src="/machinelearning-blog/assets/images/2019-11-08-Multi-Task-Deep-Neural-Networks-For-Natural-Language-Understanding-2.png" alt="image-3" /><br />
<img src="/machinelearning-blog/assets/images/2019-11-08-Multi-Task-Deep-Neural-Networks-For-Natural-Language-Understanding-3.png" alt="image-4" /><br />
<img src="/machinelearning-blog/assets/images/2019-11-08-Multi-Task-Deep-Neural-Networks-For-Natural-Language-Understanding-4.png" alt="image-5" /></p>

<h2 id="experiments">Experiments</h2>
<ol>
  <li>GLUE<br />
collection of nine NLU tasks이며, MLT에 주로 이용한 데이터</li>
  <li>SNLI<br />
sentence pair가 들어가있으며 주로 entailmnent데이터로 쓰인다, 해당 논문에서는 domain-adaptation으로만 쓰였다. (fine tuning하여 BERT와 성능비교시에)</li>
  <li>SCiTail<br />
과학 질문과 그에 대한 예상답변과 전제를 맞추는 데이터셋. 이 또한 domain-adaptation으로만 쓰였다.</li>
</ol>

<h2 id="datasets">DATASETs</h2>
<ol>
  <li>CoLA (Corpus of Linguistic Acceptability)<br />
영어 문장이 언어학적으로 맞는지 아닌지 예측</li>
  <li>SST-2 (Stanford Sentiment Treebank)<br />
Sentiment of Sentence을 결정. 영화 리뷰에서의 인간의 반응(긍부정)</li>
  <li>STS-B (Semantic Textual Similarity Benchmark)<br />
뉴스 헤드라인, 비디오, 이미지 캡션에 대해 얼마나 유사한지를 1~5점사이의 값으로 데이터가 제공되어 있음.</li>
  <li>QNLI (Qeustion-Answering NLI)<br />
원래 이 데이터세트는 SQuAD의 (변형?)한 버전으로 Question에 Answer을 할 수 있을지 없는지를 맞추는 데이터인데 여기서는 Relevance ranking task로 사용하였다.<br />
Paragraph-Question이 쌍이고 이 때, 답을 할 수 있냐 없냐를 맞추는 문제</li>
  <li>QQP (Quora Question Pairs)<br />
커뮤니티의 question-answering website Quora에서 추출한 데이터<br />
두 개의 질문이 같은 의미를 갖는지 0, 1로 표시</li>
  <li>MRPC (Microsoft Research Paraphrase Corpus)<br />
QQP와 유사하고 온라인 뉴스로 부터 문장을 뽑았음.<br />
Accuract와 F1측정 방법이 evaluation metric임</li>
  <li>MNLI (Multi-Genre Natural Language Inference)<br />
Large-scale로 crowd-source entailment classificatino task임.<br />
P가 주어질 때, H가 Entailment, conradiction, neutral인지를 맞추는 거임.</li>
  <li>RTE (Recognizing Textual Entailment)<br />
Series of annual challenges on textual entailment의 데이터 모음?<br />
MNLI와 비슷하나 entailment / not entailment 두 가지로 구분</li>
  <li>WNLI (Winograd NLI)<br />
Winograd Schema dataset으로부터 추출된 NLI 데이터 세트<br />
대명사(pronoun)가 주어진 문장에서 선택목록의 대명사 대상을 선택하는 것</li>
  <li>SNLI (Stanford NLI)<br />
NLI 데이터세트에서 엄청 보편적으로 사용되는 것<br />
이 논문에서 Domain adaptation을 위한 데이터세트로 사용<br />
SciTail (Science entailment)<br />
Science question Answering(SciQ)에서부터 추출된 entailment dataset<br />
P가 H를 entail 하는지 맞추는 문제.<br />
과학적인 문장들이기 때문에 challenge 한 듯<br />
이 논문에서 Domain adaptation을 위한 데이터세트로 사용  <br />
(출처: https://ai-information.blogspot.com/2019/02/multi-task-deep-neural-networks-for.html by rungjoo)</li>
</ol>

<h2 id="glue-results">GLUE Results</h2>
<p>MT-DNN_fine_tuning은 fine tuning없이 task를 유추하는데 쓰인 모델<br />
ST-DNN은 MTL없이 single task fine tuning을 진행한 모델<br />
ST-DNN과 MT-DNN을 비교함을 통해서 MT-DNN, 즉 MTL이 높은 성능향상 효과를 가져다줌을 알 수 있다.<br />
<img src="/machinelearning-blog/assets/images/2019-11-08-Multi-Task-Deep-Neural-Networks-For-Natural-Language-Understanding-10.png" alt="image-10" /><br />
<img src="/machinelearning-blog/assets/images/2019-11-08-Multi-Task-Deep-Neural-Networks-For-Natural-Language-Understanding-11.png" alt="image-11" /></p>

<h2 id="domain-adaptation-results-on-snli-and-scitail">Domain Adaptation Results on SNLI and SciTail</h2>
<p>BERT, MT-DNN각각에 Training Data percentage을 0.1, 1, 10, 100으로 두어서 각 모델에 fine-tuning data크기가 얼마나 영향을 주는지 확인해본다. 실험결과 MT-DNN은 매우적은 데이터로도 큰 성능을 내며, 데이터가 커질수록 그 차이가 매워지나, MT-DNN이 여전이 성능이 좋음을 확인할 수 있다.<br />
<img src="/machinelearning-blog/assets/images/2019-11-08-Multi-Task-Deep-Neural-Networks-For-Natural-Language-Understanding-12.png" alt="image-12" /></p>

<h2 id="conclusion">Conclusion</h2>
<p>BERT에 MTL을 이용해서 unsupervised learning을 이용한 BERT보다 generative하고, 성능을 향상 시켜주는 모델을 만든다.<br />
labeled data가 많이 없을 때, MT-DNN을 통해 representation을 뽑으면 매우 좋은 성능을 가질 수 있다.<br />
두개의 문장을 비교할 때는 SAN을 써보자!</p>


            <!-- Comments -->
            
<div class="comments">
    <div id="disqus_thread"></div>
    <script>
        /**
         * RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
         * LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
         */
        /*
         var disqus_config = function () {
         this.page.url = PAGE_URL; // Replace PAGE_URL with your page's canonical URL variable
         this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
         };
         */
        (function() { // DON'T EDIT BELOW THIS LINE
            var d = document, s = d.createElement('script');

            s.src = '//my_disque_settings/embed.js';

            s.setAttribute('data-timestamp', +new Date());
            (d.head || d.body).appendChild(s);
        })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>
</div>


        </div>
        

    </div>

</article>

        </div>

    <footer class="container">

    <div class="site-footer">

        <div class="copyright pull-left">
            <!-- Please keep this line to let others know where this theme comes from. Thank you :D -->
            Power by <a href="https://github.com/psi9730/development-blog">psi9730</a>
        </div>

        <a href="https://github.com/psi9730" target="_blank" aria-label="view source code">
            <span class="mega-octicon octicon-mark-github" title="GitHub"></span>
        </a>

        <div class="pull-right">
            <a href="javascript:window.scrollTo(0,0)" >TOP</a>
        </div>

    </div>

    <!-- Third-Party JS -->
    <script type="text/javascript" src="/machinelearning-blog/bower_components/geopattern/js/geopattern.min.js"></script>

    <!-- My JS -->
    <script type="text/javascript" src="/machinelearning-blog/assets/js/script.js"></script>

    

    

</footer>


    </body>

</html>
