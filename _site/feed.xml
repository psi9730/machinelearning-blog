<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Seungil&#39;s development Blog</title>
    <description>Computer Science Tech</description>
    <link>http://psi9730.github.io/machinelearning-blog/</link>
    <atom:link href="http://psi9730.github.io/machinelearning-blog/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Thu, 07 Nov 2019 14:17:03 +0900</pubDate>
    <lastBuildDate>Thu, 07 Nov 2019 14:17:03 +0900</lastBuildDate>
    <generator>Jekyll v3.0.1</generator>
    
      <item>
        <title>BERT</title>
        <description>&lt;p&gt;Paper review on “BERT: Pre-training of Deep Bidirectional Transformers for&lt;br /&gt;
Language Understanding(2018-10-11)”&lt;/p&gt;

&lt;h2 id=&quot;abstraction&quot;&gt;abstraction&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Bidirectional Encoder Representations from Transformer&lt;/li&gt;
  &lt;li&gt;wiki + BooksCorpus(total 3300M words) unlabeled data pre-training and labeled data transfer learning&lt;/li&gt;
  &lt;li&gt;masked language models(MLM), next sentence prediction(NSP) on pre-training&lt;/li&gt;
  &lt;li&gt;BERT advances the state of the art for eleven NLP tasks&lt;/li&gt;
  &lt;li&gt;with few architecture change, small fine-tuning data and epochs&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=&quot;/assets/files/BERT_ Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf&quot;&gt;BERT-PDF-download&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Wed, 06 Nov 2019 00:00:00 +0900</pubDate>
        <link>http://psi9730.github.io/machinelearning-blog/nlp/2019/11/06/BERT.html</link>
        <guid isPermaLink="true">http://psi9730.github.io/machinelearning-blog/nlp/2019/11/06/BERT.html</guid>
        
        <category>nlp</category>
        
        
        <category>NLP</category>
        
      </item>
    
      <item>
        <title>NAVER SPEECH HACKATHON 2019</title>
        <description>&lt;p&gt;Experience attending “NAVER AI HACKATHON SPEECH(2019-09-17~ 2019-10-13)”&lt;/p&gt;

&lt;h2 id=&quot;section&quot;&gt;참가 후기&lt;/h2&gt;
&lt;p&gt;첫번째 AI해커톤인만큼 설레고 재밌는 경험이었다. 처음엔 STT에 분야에 대한 사전 지식이 없어서, 음성을 데이터화 시키는 mel spectogram 부터 이를 처리하는 다양한 모델들을 &lt;a href=&quot;https://arxiv.org/abs/1508.01211&quot;&gt;listen and spell&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/1706.03762&quot;&gt;attention is all you need&lt;/a&gt;, &lt;a href=&quot;http://proceedings.mlr.press/v48/amodei16.pdf&quot;&gt;deep speech 2&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/pdf/1211.3711.pdf&quot;&gt;RNN Transducer&lt;/a&gt;을 차근차근 공부했고, 그중 구조화가 쉬우며 간단한 음성 input에 대해 performance가 좋은 seq2seq 모델을 선택했다.&lt;/p&gt;

&lt;p&gt;논문을 읽는 것과 실제로 코드를 구현하는것은 매우 재밌으면서 힘들다는 것을 느꼈고, 특히 성능를 올리는 방법을 고민할 때는 지식의 부족함을 크게 느꼈다. 오프라인 당일 날에는 24시간 시간이 주어졌지만 트레이닝 시간만 18시간 넘게 들어가는 상황에서 크게 성능을 향상 시킬수 없을 것이라 예상했다. 하지만 hackathon 당일날 페어코딩으로 상당한 집중력을 발휘 할 수 있었고, 튜터님들의 조언과 함께 성능을 대폭 상승시키는 행복한 경험을 할 수 있었다. 당일날에는 트레이닝 된 모델을 활용하는 beam search, ensemble에 초점을 맞춰서 코딩을 진행했다.&lt;/p&gt;

&lt;p&gt;끝까지 최선을 다했고, 9등이라는 아쉽지만 뜻깊었던 결과를 맞이할 수 있었다. 이후 1,2,3위 팀의 질의응답시간에는 구현 방식에 대한 많은 질문을 했었다. 대회를 진행하면서 계속 성능의 차이가 모델의 구조에서 크게 올것이라 생각했는데 data preprocessing, weight initialization에서 놓친 부분이 많았었다는 사실이 가장 기억에 남는다. 어떤 작업이든 기본 실력을 쌓는 것이 가장 중요하다라는 것을 크게 느꼈다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/hackathon-image.jpeg&quot; alt=&quot;HAPPY HACKATHON!&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;section-1&quot;&gt;참고자료&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/elzino/naver_ai_hackathon_speech&quot;&gt;행복코딩’s HACKATHON GITHUB REPO&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Tue, 05 Nov 2019 00:00:00 +0900</pubDate>
        <link>http://psi9730.github.io/machinelearning-blog/stt/2019/11/05/Naver-Speech-Hackathon.html</link>
        <guid isPermaLink="true">http://psi9730.github.io/machinelearning-blog/stt/2019/11/05/Naver-Speech-Hackathon.html</guid>
        
        <category>stt</category>
        
        
        <category>STT</category>
        
      </item>
    
      <item>
        <title>Character-Level Language Modeling with Deeper Self-Attention</title>
        <description>&lt;p&gt;Paper review on “Character-Level Language Modeling with Deeper Self-Attention(2018-08-09)”&lt;/p&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;LSTM, RNN 의 성능은 long-term context를 기억하고 있다는데에 있다. 이번 논문에서는 fixed context deep(64-layer) transformer model에 character level language modeling을 적용해 문장예측에 높은 성능을 가짐을 보인다. deep layer을 이용하는만큼 speed convergence, additional regularizer기능을 제공해주는 auxiliary losses(Multiple Positions, Intermediate Layer Losses, Mulitple Targets)을 제안한다.&lt;/p&gt;

&lt;h2 id=&quot;character-transformer-model&quot;&gt;Character Transformer model&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/character-level-language-modeling-with-deeper-self-attention-1.png&quot; alt=&quot;image-1&quot; /&gt;&lt;br /&gt;
language 모델은 다음과 같은 방식으로 예측 sequence probability을 예측하며, 64 transformer layers을 이용하며 각 포지션이 leftward하게 영향을 받기 위해 Figure 1 처럼 mask을 적용시킨다.&lt;br /&gt;
&lt;img src=&quot;/assets/images/character-level-language-modeling-with-deeper-self-attention-5.png&quot; alt=&quot;image-2&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;auxiliary-losses&quot;&gt;Auxiliary Losses&lt;/h2&gt;
&lt;p&gt;네트워크가 10 layers이상 깊어지면 slow convergence 와 poor accuracy문제가 발생하게 된다. 이에 본 논문에서는 다음과 같은 auxiliary losses을 제안한다. 이는 speed up convergence와 additional regularizer 역할을 한다.&lt;br /&gt;
각각의 auxiliary losses는 total loss of network에 더해지게 되며, 상대적으로 낮은 weight을 가지며. 각 losses마다 own schedule of decay 를 가진다.&lt;/p&gt;

&lt;h3 id=&quot;mulitple-positions&quot;&gt;Mulitple Positions&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/character-level-language-modeling-with-deeper-self-attention-2.png&quot; alt=&quot;image-3&quot; /&gt;&lt;br /&gt;
예측을 마지막 레이어 마지막 값에서 마지막 레이어 모든 값으로 바꾼다. 이는 때때로 하나 또는 두개의 단어로 다음 단어를 예측해야하는 상황을 만들지만, 실험적으로 이 loss정책은 트레이닝 속도를 올리고 더 좋은 결과를 가지게 해준다.&lt;/p&gt;

&lt;h3 id=&quot;intermediate-layer-losses&quot;&gt;Intermediate Layer Losses&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/character-level-language-modeling-with-deeper-self-attention-3.png&quot; alt=&quot;image-4&quot; /&gt;&lt;br /&gt;
final layer을 제외하고 lower layers에도 predictions을 구하고 loss을 만드는 것을 말한다. lower layers는 상대적으로 낮은 contribute을 가지게 되며, l^th intermediate layer는 l/2n of training이후에 loss에 영향을 주지 않게 된다.&lt;/p&gt;

&lt;h3 id=&quot;multiple-target&quot;&gt;Multiple Target&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/character-level-language-modeling-with-deeper-self-attention-4.png&quot; alt=&quot;image-5&quot; /&gt;&lt;br /&gt;
예측시에 하나의 값이 아닌 multiple한 값을 예측한다. extra targets 의 weight는 0.5라는 값을 곱해서 loss을 만들게 구현한다.&lt;/p&gt;

&lt;h2 id=&quot;positional-embedding&quot;&gt;Positional Embedding&lt;/h2&gt;
&lt;p&gt;64 layers는 매우 깊음으로 sinusodial timing singal을 input에서만 더하는 것으로는, propagation하면서 그 기능이 미미해 질 수 있다 판단했다.이에 본 논문에서는 learned positional embedding을 각 transformer layer input sequence에 더하게 된다. 이 때 layer 마다 positional embedding은 공유하지 않는다.&lt;/p&gt;

&lt;h2 id=&quot;result&quot;&gt;Result&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/character-level-language-modeling-with-deeper-self-attention-5.png&quot; alt=&quot;image-6&quot; /&gt;&lt;br /&gt;
character and accuracy of model is best on layer 64 tranformers. 그리고 적용시킨 auxiliary losses에 대해 ablation experiments을 한 결과. 모든 auxiliary losses을 고려했을 때 최상의 결과를 가져 왔다. 또한 같은 모델을 word모델에 적용시 PPL이 word level model이 더낮게 나왔는데 이는 추가 연구가 필요할 것 같다.&lt;/p&gt;

&lt;p&gt;또한 분석결과 character level model이 prefer actual English words over non-existent words함을 확인했고, transformer self-attention구조가 copy sequences over long distances능력을 가져다줌을 예상한다.&lt;/p&gt;

&lt;h2 id=&quot;bpc&quot;&gt;bpc란?&lt;/h2&gt;
&lt;p&gt;After all input has been read and encoded, the total length (in bits) of the result is measured and divided by the number of characters in the original, uncompressed input. If the model is good, it will have predicted the characters with high accuracy, so the bit sequence used for each character will have been short on average, hence the total bits per character will be low.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;[what-is-bpc] (https://stackoverflow.com/questions/17797922/how-to-calculate-bits-per-character-of-a-string-bpc)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section&quot;&gt;참고문헌&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1803.02155&quot;&gt;Character-Level Language Modeling with Deeper Self-Attention&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Wed, 23 Oct 2019 00:00:00 +0900</pubDate>
        <link>http://psi9730.github.io/machinelearning-blog/nlp/2019/10/23/Character-Level-Language-Modeling-With-Deeper-Self-Attention.html</link>
        <guid isPermaLink="true">http://psi9730.github.io/machinelearning-blog/nlp/2019/10/23/Character-Level-Language-Modeling-With-Deeper-Self-Attention.html</guid>
        
        <category>nlp</category>
        
        
        <category>NLP</category>
        
      </item>
    
      <item>
        <title>Self-Attention with Relative Position Representations</title>
        <description>&lt;p&gt;Paper review on “Self-Attention with Relative Position Representations(2018-03-06)”&lt;/p&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;transformer은 recurrent, convolutional neural network와 달리 모델 자체에서 order of sequence을 표현하지 못하는 단점이 존재한다. 이에 sinusoidal singal를 이용해서 absolute position을 input에 더해주는 식의 position embedding을 활용하게 되었다. 이번 논문에서는 이러한 position을 relative하게 representation하는 방법을 제안한다. self-attention mechanism에 relative position 또는 distance between sequence elements을 추가했으며 이 방식이 machine translation에서 performance을 증가시킴을 확인했다. 또한 학습 중에 추가한 relative position을 구하며, 모든 heads에 동일한 relative position을 적용함으로써 space complexity을 줄였다.&lt;/p&gt;

&lt;h2 id=&quot;proposed-architecture&quot;&gt;Proposed Architecture&lt;/h2&gt;
&lt;p&gt;### Relation-aware Self-Attention&lt;br /&gt;
positional information을 담기 위해 input elements간에 pairwise relationships을 이용한다. x_i, x_j 사이의 edge의 종류를 a_i_j^V, a_i_j^K 두가지 종류로 둔다. &lt;br /&gt;
&lt;img src=&quot;/assets/images/self-attention-with-relative-position-representations-1.png&quot; alt=&quot;image-1&quot; /&gt;&lt;br /&gt;
&lt;img src=&quot;/assets/images/self-attention-with-relative-position-representations-2.png&quot; alt=&quot;image-2&quot; /&gt;&lt;br /&gt;
3번 식은 주어진 attention head에 선택된 값이 다음 encoder or decoder layers에게 유용한가를 구할 때 유용하다.&lt;br /&gt;
4번 식은 attention head값을 변화시켜주는 값이다. 두가지 pairwise information을 더해줌으로써 additional linear transformations 없이 relation을 나타낼 수 있다.&lt;/p&gt;

&lt;h3 id=&quot;relative-position-representations&quot;&gt;Relative Position Representations&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/self-attention-with-relative-position-representations-3.png&quot; alt=&quot;image-3&quot; /&gt;&lt;br /&gt;
&lt;img src=&quot;/assets/images/self-attention-with-relative-position-representations-4.png&quot; alt=&quot;image-4&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Input elements간의 거리가 일정이상 커지면 relation position information이 크게 유용하지 않다는 가정하에 다음과 같은 clip구조를 제안한다. 이에 edge label은 2k+1 unique가 있으며, 이를 통해 sequence length에 대한 고려를 포함시킬 수 있다. 해당 논문에서는 k=64에서 최대 BLEU을 가지는 것이 확인된다.&lt;/p&gt;

&lt;h3 id=&quot;efficient-implementation&quot;&gt;Efficient Implementation&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/self-attention-with-relative-position-representations-5.png&quot; alt=&quot;image-5&quot; /&gt;&lt;br /&gt;
By relative position representations sharing across attention heads, storing relative position representations space complexity = O(h&lt;em&gt;d_a&lt;/em&gt;n^2) to O(d_a&lt;em&gt;n^2). By relative position representations sharing across sequences, all space complexity = O(bhnd) to O(bhnd + d_a&lt;/em&gt;n^2)&lt;br /&gt;
그리고 식 4번을 5번 식으로 바꿈으로써 parallel multiplcation이 가능하도록 한다.&lt;/p&gt;

&lt;h2 id=&quot;section&quot;&gt;참고문헌&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1706.03762&quot;&gt;Self-Attention with Relative Position Representations&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</description>
        <pubDate>Tue, 22 Oct 2019 00:00:00 +0900</pubDate>
        <link>http://psi9730.github.io/machinelearning-blog/nlp/2019/10/22/Self-Attention-With-Relative-Position_representations.html</link>
        <guid isPermaLink="true">http://psi9730.github.io/machinelearning-blog/nlp/2019/10/22/Self-Attention-With-Relative-Position_representations.html</guid>
        
        <category>nlp</category>
        
        
        <category>NLP</category>
        
      </item>
    
      <item>
        <title>Univeral Sentence Encoder</title>
        <description>&lt;p&gt;Paper review on “Universal Sentence Encoder(2018-3-29)”&lt;/p&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;NLP에선 워드 레벨의 pretrained Embedding을 활용한 성능향상 연구가 있어왔는데 본 논문에서는 transformer, DAN(Deep Averaging Network) 모델의 sentence Embedding을 제시함. 동시에 transfer learning에 활용하여 다양한 분야에 적은 수의 트레이닝 데이터셋으로 우수한 성능을 낼 수 있음을 보임.&lt;/p&gt;

&lt;h2 id=&quot;sentence-embeddingencoder&quot;&gt;sentence Embedding(Encoder)&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;Transformer&lt;br /&gt;
transformer의 Encoder sub-graph는 단어들간의 순서와 문맥 정보를 포함시키기 위해 attention을 활용함&lt;/li&gt;
  &lt;li&gt;DAN(Deep Averaging Network)&lt;br /&gt;
Averaging words/bi-grams embedding 이후 feedforward DNN에 태워 임베딩 벡터를 생산, Transformer에 비해 문장 길이에 대한 시간복잡도가 linear하게 증가한다는 장점이 있음. 정확도는 Transformer가 대체적으로 더 높게 나옴.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;encoders&quot;&gt;encoders&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;인코더는 PTB(Penn Tree Bank) tokenized string을 인풋으로 받고 512-d 벡터(sentence Embedding)을 반환&lt;/li&gt;
  &lt;li&gt;Multi-task learning을 위해 Skip-Thought like task, Conversational input-response task, Classifications등의 다운 스트림 task에 문장 임베딩을 얻는 학습을 시킴.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/universal-sentence-encoder.png&quot; alt=&quot;universal-sentence-encoder&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;transfer-learning&quot;&gt;Transfer Learning&lt;/h2&gt;
&lt;p&gt;word level Transfer/ no transfer learning의 두 모델을 베이스라인으로 삼음.&lt;br /&gt;
sentence Embedding, word embedding, sentence embedding+word embedding(concat)을 이용하여 transfer learning성능을 비교함.&lt;br /&gt;
### Transfer Learning Result&lt;br /&gt;
Transformer &amp;gt; DAN&lt;br /&gt;
Sentence&amp;amp;word embedding &amp;gt; Sentence embedding &amp;gt; word embedding&lt;br /&gt;
데이터가 작을 수록 sentence level transfer learning성능이 word level 대비 매우 우수&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1803.11175&quot;&gt;Universal Sentence Encoder&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Sun, 20 Oct 2019 00:00:00 +0900</pubDate>
        <link>http://psi9730.github.io/machinelearning-blog/nlp/2019/10/20/Universal-Sentence-Encoder.html</link>
        <guid isPermaLink="true">http://psi9730.github.io/machinelearning-blog/nlp/2019/10/20/Universal-Sentence-Encoder.html</guid>
        
        <category>nlp</category>
        
        
        <category>NLP</category>
        
      </item>
    
      <item>
        <title>Attention is all you need</title>
        <description>&lt;p&gt;Paper review on “attention is all you need(2017-11-6)”&lt;/p&gt;

&lt;h2 id=&quot;transformer&quot;&gt;트랜스포머(transformer)란?&lt;/h2&gt;
&lt;p&gt;Attention mechanism을 기반으로 complex recurrent or convolutional neural network기반 sequence transduction model보다 parallelizable 하게 실행시키기 용의하며 트레이닝 시간을 명확히 줄일 수 있다. 기존 encoder-deocoder기반 sequence 모델은 다음 예측을 위해 그전 hidden_state과 input이 필요하며 이에 의해 input sequence가 늘어날 수록 시간, 메모리 활용면에서 문제가 난타난다. 또한 단어간 시간 차이가 클수록 영향력이 작아지는 문제가 있는데 이를 해결하기 위해 attention 개념이 등장했다. transformer는 self-attention을 이용하여 input, output간의 global dependency를 잡아냈으며, scaled dot-product, multi head attention을 이용하여 parallel processing에 장점을 가진다.&lt;/p&gt;

&lt;h2 id=&quot;seq2seq-attention&quot;&gt;Seq2Seq모델에서 attention이란?&lt;/h2&gt;
&lt;p&gt;디코더의 특정 time-step의 output이 인코더의 모든 time-step의 output 중 어떤 time-step과 가장 연관이 있는가&lt;/p&gt;

&lt;h2 id=&quot;self-attention&quot;&gt;self-attention이란?&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;Scaled Dot-Product Attention&lt;br /&gt;
Scaled Dot-Product는 Q와 K 을 내적하여 어텐션을 Softmax를 통해 구하고, 그 후에 V에 내적하여 중요한 부분(Attention)에 따른 V값을 구하는 것을 말한다. 이 때 Self Attention이란 Q, K, V에 같은 벡터를 넣어서 같은 문장 내에서 상호 중요도를 평가한다고 볼 수 있다.&lt;br /&gt;
(base attention function엔 additive와 dot product attention이 있다고 한다.)&lt;/li&gt;
  &lt;li&gt;Multi-Head attention&lt;br /&gt;
heads의 수만큼 다른 관점으로 attention을 구한 다음 이를 concatenate하여 최종 Attention을 구한다. 하나의 문장의 의미를 다양한 시각으로 해석할 수 있음을 적용한 개념.&lt;/li&gt;
&lt;/ol&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-css&quot; data-lang=&quot;css&quot;&gt;    &lt;span class=&quot;nt&quot;&gt;MultiHead&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;Q&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;K&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;V&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;Concat&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;head_1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;...,&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;head_h&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;^&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;O&lt;/span&gt;
        &lt;span class=&quot;nt&quot;&gt;where&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;head_i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;Attention&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;QW_i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;^&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;Q&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;KW_i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;^&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;K&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;VW_i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;^&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;V&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;ol&gt;
  &lt;li&gt;position-wise feed-forward networks&lt;br /&gt;
Convolution layer을 이용해 concatenate된 multi-head attention을 균등하게 섞는 역할을 한다. 기존 attention은 head에따라 편향된 attention을 가지고 있음으로 이를 해결하기 위해 position-wise networks를 이용한다.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;section&quot;&gt;이외 주요개념&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;positional encoding&lt;br /&gt;
rnn의 장점중하나는 input의 순서를 기억하고 결과에 반영하는 것이 있다. 본 논문에서는 input에 이러한 order of sequence을 반영하기 위해 코사인, 사인 함수를 이용해서 input sequence embedding에 위치정보를 반영했다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Layer Normalization&lt;br /&gt;
sub-layer가 residual connection으로 연결, 그 후에는 layer-normalization과정을 거침&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Residual dropout&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Label smoothing&lt;br /&gt;
## 참고문헌&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1706.03762&quot;&gt;Attnetion Is All You Need&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

</description>
        <pubDate>Thu, 17 Oct 2019 00:00:00 +0900</pubDate>
        <link>http://psi9730.github.io/machinelearning-blog/nlp/2019/10/17/Attention-Is-All-You-Need.html</link>
        <guid isPermaLink="true">http://psi9730.github.io/machinelearning-blog/nlp/2019/10/17/Attention-Is-All-You-Need.html</guid>
        
        <category>nlp</category>
        
        
        <category>NLP</category>
        
      </item>
    
  </channel>
</rss>
